---
#
# By default, content added below the "---" mark will appear in the home page
# between the top bar and the list of recent posts.
# To change the home page layout, edit the _layouts/home.html file.
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
#
layout: home
---

<!-- <img src="/2023/images/deep.jpg"> -->
<center>
<h2 class="blackpar_title"> The 5th New Frontiers in Summarization Workshop </h2>
<h3 class="blackpar_title"> EMNLP 2025 </h3>
</center>
The Fifth Workshop on "New Frontiers in Summarization" aims to foster cross-fertilization of ideas in automatic summarization and related fields. It will cover novel paradigms, shared tasks, applied research, and future directions while accelerating the development of tools, datasets, and resources to meet the summarization needs of academia, industry, and government. As advances in natural language processing (e.g., pre-trained models and prompt-based learning) improve summarization performance, challenges remain in areas such as trustworthiness, interpretability, evaluation reliability, and the integration of knowledge and modalities for real-world deployment.

To tackle these challenges, we plan to expand the workshop's scope beyond traditional summarization to include grounded text generation with retrieval, reference- and attribute-based summarization, multi-modal and long-form summarization, query-focused approaches, hallucination reduction, efficiency, and novel evaluation methods. This broader focus, particularly addressing the growing role of large language models (LLMs), is expected to attract wider engagement from the research community and push the boundaries of summarization research.


<!-- Confirmed Spearkers -->
<h2 class="blackpar_title" id="Confirmed Spearkers">Keynote Spearkers</h2>


<div class="row" style="display: flex;">
  <!-- Speaker's Details Column (Narrower) -->
  <div class="column left" style="flex: 1; max-width: 25%;">
    <div class="card">
      <img src="/newsumm_2025/images/mb2019.png" alt="Mohit Bansal" class="img_card" style="width: 100%;">
      <div class="container">
        <h2><a href="https://www.cs.unc.edu/~mbansal/"><b>Mohit Bansal</b></a></h2>
        <p class="affiliation">University of North Carolina at Chapel Hill</p>
      </div>
    </div>
  </div>
  
  <!-- Title and Abstract Column (Wider) 
  <div class="column right" style="flex: 3; max-width: 75%;">
    <div class="container">
      <h3 class="talk_title">Addressing Large Language Models that Lie: Case Studies in Summarization</h3>
      <p class="abstract">The advent of large language models promises a new level of performance in the generation of text of all kinds, enabling the generation of text that is far more fluent, coherent, and relevant than was previously possible. However, they also introduce a major new problem: they hallucinate facts out of thin air. When summarizing an input document, they may incorrectly intermingle facts from the input, they may introduce facts that were not mentioned at all, and worse yet, they may even make up things that are not true in the real world. In this talk, I will discuss our work in characterizing the kinds of errors that can occur and methods that we have developed to help mitigate hallucination in language modeling approaches to text summarization for a variety of genres.</p>
    </div>
  </div>
</div>
-->

<div class="row" style="display: flex;">
  <!-- Speaker's Details Column (Narrower) -->
  <div class="column left" style="flex: 1; max-width: 25%;">
    <div class="card">
      <img src="/newsumm_2025/images/arman.jpg" alt="Arman Cohan" class="img_card" style="width: 100%;">
      <div class="container">
        <h2><a href="https://armancohan.com/"><b>Arman Cohan</b></a></h2>
        <p class="affiliation">McGill University</p>
      </div>
    </div>
  </div>
  
  <!-- Title and Abstract Column (Wider)
  <div class="column right" style="flex: 3; max-width: 75%;">
    <div class="container">
      <h3 class="talk_title">Open Problems in Automatic Summarization</h3>
      <p class="abstract">Pre-trained language models have met and exceeded human-level performance on summarization benchmarks, often with the help of adaptation towards task-specific or human-elicited rewards. What does this mean for the field of automatic summarization? I argue that these results represent a milestone to be celebrated, but that they barely scratch the surface of the work ahead for summarization researchers. I discuss challenges that still remain largely unsolved and under-researched: how do we develop summarization systems that can perform more complex reasoning? How do we use this reasoning capability to aggregate and analyze information across vast amounts of text? What responsible AI issues matter in the deployment of summarization systems? And how do we evaluate for all of these desiderata? Progress on these open questions gives us the exciting prospect of summarization systems that are useful and beneficial in practice.</p>
    </div>
  </div>
</div>

 -->

<div class="row" style="display: flex;">
  <!-- Speaker's Details Column (Narrower) -->
  <div class="column left" style="flex: 1; max-width: 25%;">
    <div class="card">
      <img src="/newsumm_2025/images/greg.png" alt="Greg Durrett" class="img_card" style="width: 100%;">
      <div class="container">
        <h2><a href="https://www.cs.utexas.edu/~gdurrett/"><b>Greg Durrett</b></a></h2>
        <p class="affiliation">The University of Texas at Austin</p>
      </div>
    </div>
  </div>
  
  <!-- Title and Abstract Column (Wider)
  <div class="column right" style="flex: 3; max-width: 75%;">
    <div class="container">
      <h3 class="talk_title">Are Large Language Models Fair Summarizers? </h3>
      <p class="abstract">We live in a world of value pluralism where several values can be equally correct and fundamental, and yet in conflict with each other. Traditional summarization models are optimized for the prestige and centrality of important words and concepts but not for diversity and novelty, and thus when presented with diverse perspectives and conflicting opinions, they can display bias by ignoring certain parts of inputs. In this talk, I will present our initial efforts in investigating fair abstractive summarization by large language models that aims to generate a fair summary for user-generated data by providing an accurate and comprehensive view of various perspectives from different groups.</p>
    </div>
  </div>
</div>
 -->


<div class="row" style="display: flex;">
  <!-- Speaker's Details Column (Narrower) -->
  <div class="column left" style="flex: 1; max-width: 25%;">
    <div class="card">
      <img src="/newsumm_2025/images/fabbri.png" alt="Alexander R. Fabbri" class="img_card" style="width: 100%;">
      <div class="container">
        <h2><a href="https://alex-fabbri.github.io/"><b>Alexander R. Fabbri</b></a></h2>
        <p class="affiliation">Allen Institute for AI</p>
      </div>
    </div>
  </div>
  
  <!-- Title and Abstract Column (Wider) 
  <div class="column right" style="flex: 3; max-width: 75%;">
    <div class="container">
      <h3 class="talk_title">The Quest for Open Language Models</h3>
      <p class="abstract">With the rapid progress of proprietary language models, the open research community is spending more effort to advance the state-of-the-art of open models. For that, AI2 started a project called OLMo, aiming to build and release an entirely in-house, truly open LLM. In this talk, I will tell you more about what AI2 is building for OLMo. I will discuss the DOLMA dataset and its toolkit. I will discuss our pretraining and modeling experience. Finally, I will talk about our efforts in language model adaptation, focusing on instruction tuning and RLHF efforts to train and evaluate the TULU models. 

With the rapid advancement of proprietary language models, the open research community is increasingly focused on developing state-of-the-art open models. AI2 has initiated a project named OLMo, with the goal of creating and releasing a completely in-house, fully open LLM. In this presentation, I will delve into AI2's work on OLMo. I'll explore the DOLMA dataset and its associated toolkit, share insights from our pretraining and modeling experiences, and conclude by discussing our language model adaptation efforts. This will focus on instruction tuning and RLHF efforts used to train and evaluate the TULU models.</p>
    </div>
  </div>
</div>
-->

<div class="row" style="display: flex;">
  <!-- Speaker's Details Column (Narrower) -->
  <div class="column left" style="flex: 1; max-width: 25%;">
    <div class="card">
      <img src="/newsumm_2025/images/yulia.jpg" alt="Yulia Tsvetkov" class="img_card" style="width: 100%;">
      <div class="container">
        <h2><a href="https://homes.cs.washington.edu/~yuliats/"><b>Yulia Tsvetkov</b></a></h2>
        <p class="affiliation">Zoom</p>
      </div>
    </div>
  </div>
  
  <!-- Title and Abstract Column (Wider) 
  <div class="column right" style="flex: 3; max-width: 75%;">
    <div class="container">
      <h3 class="talk_title">Facing the Challenges and Opportunities of LLMs</h3>
      <p class="abstract">Recent progress of LLMs has dramatically changed the research and business communities, and posed challenges to many NLP research areas such as summarization. However, the ultimate goal of any model is to perfectly solve specialized domain problems users care about, which is not achieved by existing LLMs as a generalist. In this talk, I will introduce how to face these challenges and adapt NLP research in the new LLM era. This includes: 1) Adapt and Improve, i.e., integrate traditional NLP and ML techniques with LLM to achieve better performance in various domains; 2) Leverage and Empower, i.e., stand on the shoulder of LLM to tackle complex problems. These measures can effectively guide a generalist LLM towards a specialist in domains of interest. 
</p>
    </div>
  </div>
</div>
-->




<div class="row" style="display: flex;">
  <!-- Speaker's Details Column (Narrower) -->
  <div class="column left" style="flex: 1; max-width: 25%;">
    <div class="card">
      <img src="/newsumm_2025/images/marielle_lapata.jpg" alt="Mirella Lapata" class="img_card" style="width: 100%;">
      <div class="container">
        <h2><a href="https://homepages.inf.ed.ac.uk/mlap/"><b>Mirella Lapata</b></a></h2>
        <p class="affiliation">Zoom</p>
      </div>
    </div>
  </div>
  
  <!-- Title and Abstract Column (Wider) 
  <div class="column right" style="flex: 3; max-width: 75%;">
    <div class="container">
      <h3 class="talk_title">Facing the Challenges and Opportunities of LLMs</h3>
      <p class="abstract">Recent progress of LLMs has dramatically changed the research and business communities, and posed challenges to many NLP research areas such as summarization. However, the ultimate goal of any model is to perfectly solve specialized domain problems users care about, which is not achieved by existing LLMs as a generalist. In this talk, I will introduce how to face these challenges and adapt NLP research in the new LLM era. This includes: 1) Adapt and Improve, i.e., integrate traditional NLP and ML techniques with LLM to achieve better performance in various domains; 2) Leverage and Empower, i.e., stand on the shoulder of LLM to tackle complex problems. These measures can effectively guide a generalist LLM towards a specialist in domains of interest. 
</p>
    </div>
  </div>
</div>
-->


<!-- Call for Papers -->
<h2 class="blackpar_title" id="Call for Papers">Call for Papers</h2>

Both long paper (up to 8 pages with unlimited reference) and short paper (up to 4 pages with unlimited reference) are welcomed for submission! 


A list of topics relevant to this workshop (but not limited to):

- Abstractive, extractive, and hybrid summarization
- Summarization with pre-trained large models
- Zero-shot/few-shot summarization
- Long-context summarization
- Fairness in summarization: faithfulness, bias, toxicity, and privacy-preserving methods
- Interpretability, controllability, and visualization of summarization systems
- Reference- and attribute-based summarization
- Query-focused summarization
- Knowledge-injected summarization with retrieval
- Multilingual summarization
- Multimodal summarization (text, speech, image, video)
- Multi-genre summarization (news, tweets, product reviews, conversations, medical records, etc.)
- Semantic aspects of summarization (representation, inference, validity)
- Cognitive and psycholinguistic aspects (readability, usability)
- Development of new algorithms, datasets, and annotations
- Development of new evaluation metrics
- Hallucination reduction and trustworthiness in summarization
- Efficiency in summarization and large model inference





<h2 class="blackpar_title" id="Submission">Submission Instructions</h2>
You are invited to submit your papers in our <a href='https://www.softconf.com/emnlp2025/newsumm2025/'>START/SoftConf submission portal</a>. All the submitted papers have to be anonymous for double-blind review. The content of the paper should not be longer than 8 pages for long papers and 4 pages for short papers, strictly following the <a href='https://aclrollingreview.org/authors'>ACL style templates</a>, with the mandatory limitation section not counting towards the page limit. Supplementary and appendices (either as separate files or appended after the main submission) are allowed. We encourage code link submissions for the camera-ready version.

  <h3 class="sub_section_title"><strong>Dual Submission</strong></h3>

NewSumm 2025 will allow double submission as long as the authors make a decision before camera-ready. We will not consider any paper that overlaps significantly in content or results with papers that will be (or have been) published elsewhere. Authors submitting more than one paper to NewSumm 2025 must ensure that their submissions do not overlap significantly (>25%) with each other in content or results. Authors can submit up to 100 MB of supplementary materials separately. Authors are highly encouraged to submit their codes for reproducibility purposes. 


  <h3 class="sub_section_title"><strong>Fast-Track Submission</strong></h3>
If your paper has been reviewed by ACL, EMNLP, EACL, or ARR and the average rating is higher than 2.5 (either avg soundness or excitement score), the paper is qualified to be submitted to the fast-track. In the appendix, please include the reviews and a short statement discussing what parts of the paper have been revised.


ACL Rolling Review (ARR) Submissions: Our workshop also welcomes submissions from ARR. Authors of any papers that are submitted to ARR and have their meta review ready may submit their papers and reviews for consideration for the workshop until 10 October 2025. This should include submissions to ARR for the 15 August deadline. The decision of publication will be announced by 17 October 2025. The commitment should be done via the workshop submission website: <a href='https://www.softconf.com/emnlp2025/newsumm2025/'>START/SoftConf submission portal</a> ("ACL Rolling Review Commitment" submission type)


<h3 class="sub_section_title"><strong>Non-archival Option</strong></h3>


ACL workshops are traditionally archival. To allow dual submission of work, we are also including a non-archival track. Authors have the flexibility to submit their unpublished research in a non-archival format, where only the abstract will be included in the conference proceedings. These non-archival submissions are expected to meet the same quality criteria as their archival counterparts and will undergo an identical review process. This option is designed to facilitate future publication opportunities in journals or conferences that disallow previously archived material. It also aims to foster engagement and constructive feedback on well-developed but yet-to-be-published work. Like archival submissions, non-archival entries must conform to the established formatting and length guidelines.


<h2 class="blackpar_title" id="Dates">Important Dates:</h2>

 - <strong>Sep. 1, 2025</strong>: Workshop Submission Due Date

 - <strong>Oct. 10, 2025</strong>: Fast-Track Submission and ARR Commitment Deadline 

- <strong>Oct. 17, 2025</strong>: Notification of Acceptance (Direct, ARR, and Fast-Track Notification)

 - <strong>Oct. 24, 2025</strong>: Camera-ready Papers Due

 - <strong>Dec. 6, 2025</strong>: Workshop Date



<!-- Organizers -->
<h2 class="blackpar_title" id="Organizers">Organizers</h2>
<div class="row">

 <div class="card column" >
	  <img src="/newsumm_2025/images/Yue_Dong.jpg" alt="Yue Dong" class="img_card">
	  <div class="container">
		<center>
		<h4>
      <a href="https://www.cs.mcgill.ca/~ydong26/"><b>Yue Dong</b></a>
			<br>
			University of California, Riverside, USA
		</h4>
		</center>
	  </div>
	</div>

 <div class="card column" >
	  <img src="/newsumm_2025/images/wen_xiao.jpg" alt="Wen Xiao" class="img_card">
	  <div class="container">
		<center>
		<h4>
      <a href="https://wendy-xiao.github.io/"><b>Wen Xiao</b></a>
			<br>
			Microsoft Azure AI, Canada
		</h4>
		</center>
	  </div>
	</div>
 
<div class="card column" style="margin-left:0;">
	  <img src="/newsumm_2025/images/haopeng_zhang.png" alt="Haopeng Zhang" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<a href="https://hpzhang94.github.io/"><b>Haopeng Zhang</b></a>
			<br>
			University of Hawaii at Manoa, USA
		</h4>
		</center>
	  </div>
	</div>
  </div>

<div class="row">
  <div class="card column" >
  	  <img src="/newsumm_2025/images/rui_zhang_crop.jpeg" alt="Rui Zhang" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<a href="https://ryanzhumich.github.io/"><b>Rui Zhang</b></a>
			<br>
			Penn State University, USA
		</h4>
		</center>
	  </div>
	</div>
  

<div class="card column">
	  <img src="/newsumm_2025/images/Ori_Ernst_copy.jpg" alt="Ori Ernst" class="img_card">
	  <div class="container" >
		<center>
		<h4>
			<a href="https://oriern.github.io/"><b>Ori Ernst</b></a>
			<br>
			McGill University & Mila, Canada
		</h4>
		</center>
	  </div>
	</div>

<div class="card column" style="margin-left:0;">
	  <img src="/newsumm_2025/images/lu_wang.JPG" alt="Lu Wang" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<a href="https://web.eecs.umich.edu/~wangluxy/"><b>Wang Lu</b></a>
			<br>
			University of Michigan, USA
		</h4>
		</center>
	  </div>
	</div>
  </div>

<div class="row">
  <div class="card column" >
  <img src="/newsumm_2025/images/fei_liu.JPG" alt="Fei Liu" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<a href="http://www.cs.ucf.edu/~feiliu/"><b>Fei Liu</b></a>
			<br>
			Emory University, USA
		</h4>
		</center>
	  </div>
	</div>
  
</div>


  




<!-- PC -->
<h2 class="blackpar_title" id="Program Committee">Program Committee</h2>
<ul>
<li>Shmuel Amar (Bar-Ilan University)</li>
<li>Florian Boudin (JFLI, Nantes Université)</li>
<li>Avi Caciularu (Google)</li>
<li>Arie Cattan (Bar-Ilan University)</li>
<li>Hou Pong Chan (Alibaba DAMO Academy)</li>
<li>Khaoula Chehbouni (McGill University, Mila)</li>
<li>Ziling Cheng (McGill University & Mila)</li>
<li>Jackie Cheung (Mila / McGill)</li>
<li>Maxime Darrin (Mistral AI)</li>
<li>Felice Dell'Orletta (Istituto di Linguistica Computazionale “Antonio Zampolli” (CNR-ILC))</li>
<li>Ron Eliav (Bar-Ilan University)</li>
<li>Tobias Falke (Amazon AGI)</li>
<li>Lorenzo Flores (MILA Quebec)</li>
<li>Yu Fu (University of California, Riverside)</li>
<li>Eran Hirsch (Bar-Ilan University)</li>
<li>Zhe Hu (The Hong Kong Polytechnic University)</li>
<li>Xinyu Hua (Bloomberg)</li>
<li>Patrick Huber (Meta)</li>
<li>Hayate Iso (Megagon Labs)</li>
<li>Ayal Klein (Bar Ilan University)</li>
<li>Wojciech Kryscinski (Cohere)</li>
<li>Elena Lloret (University of Alicante)</li>
<li>Margot Mieskes (University of Applied Sciences, Darmstadt)</li>
<li>Manabu Okumura (Tokyo Institute of Technology)</li>
<li>Jessica Ouyang (UT Dallas)</li>
<li>G M Shahariar (University of California, Riverside)</li>
<li>Haz Sameen Shahgir (University of California Riverside)</li>
<li>Ori Shapira (OriginAI)</li>
<li>Aviv Slobodkin (Bar Ilan University )</li>
<li>Cesare Spinoso (McGill )</li>
<li>Esaú Villatoro Tello  (Idiap Research Institute, CH)</li>
<li>David Wan (UNC Chapel Hill)</li>
<li>Haohan Yuan (ALOHA Lab, University of Hawaii at Manoa)</li>
<li>Yusen Zhang (Penn State University )</li>
<li>Nan Zhang (The Pennsylvania State University)</li>
<li>Shiyue Zhang (Bloomberg)</li>
<li>Ming Zhong (UIUC)</li>
<li>Xiyuan Zou (McGill / MILA)</li>
</ul>





