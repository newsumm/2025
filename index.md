---
#
# By default, content added below the "---" mark will appear in the home page
# between the top bar and the list of recent posts.
# To change the home page layout, edit the _layouts/home.html file.
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
#
layout: home
---

<!-- <img src="/2023/images/deep.jpg"> -->
<center>
<h2 class="blackpar_title"> The 4th New Frontiers in Summarization (with LLMs) Workshop </h2>
<h3 class="blackpar_title"> EMNLP 2023 </h3>
</center>
The Fourth Workshop on "New Frontiers in Summarization" aims to promote the cross-fertilization of ideas in automatic summarization and related fields. This includes discussion on novel paradigms, shared tasks of interest, applied research and applications, and possible future research directions. In addition to building a cohesive research community, the workshop will accelerate knowledge diffusion by developing new tools, datasets, and resources that are in line with the summarization needs of academia, industry, and government. 
		
New advances in natural language processing (e.g., pre-trained models and prompt-based learning) have resulted in state-of-the-art performance according to existing standards of summarization evaluation. A number of new challenges have emerged, and moving forward with large-scale models we don't fully understand calls for caution. Challenges are posed from multiple directions, including but not limited to the trustworthiness of the generation, the interpretability and controllability of the models, the reliability of evaluation, and the integration of additional sources like knowledge and other modality. Considering these challenges will be crucial for realistic, ecologically valid deployment of summarization research.




<!-- Call for Papers -->
<h2 class="blackpar_title" id="Call for Papers">Call for Papers</h2>

Both long paper (up to 8 pages with unlimited reference) and short paper (up to 4 pages with unlimited reference) are welcomed for submission! 


A list of topics relevant to this workshop (but not limited to):

- Abstractive summarization, extractive summarization and their integration
- Summarization with pre-trained large models
- Zero-shot/few-shot summarization
- Fairness in summarization: faithfulness, bias, toxicity, and privacy-preserving 
- Interpretability and visualization of summarization systems
- Controlled and tailored text generation
- Knowledge/common sense injected summarization
- Multiple text genres (News, tweets, product reviews, conversations, medical records, books, research articles, etc.)
- Multimodal learning: information integration and aggregation across multiple modalities (text, speech, image, video)
- Multilingual summarization
- Semantic aspects of summarization (e.g., semantic representation, inference, validity)
- Cognitive or psycholinguistic aspects of summarization (e.g., perceived readability, usability, etc.)
- Development of novel algorithms (e.g., integrating neural and non-neural, distant supervision)
- Development of new datasets and annotations
- Development of new evaluation metrics




<h2 class="blackpar_title" id="Submission">Submission Instructions</h2>
You are invited to submit your papers in our <a href='https://www.softconf.com/emnlp2023/newsumm2023/'>START/SoftConf submission portal</a>. All the submitted papers have to be anonymous for double-blind review. The content of the paper should not be longer than 8 pages for long papers and 4 pages for short papers, strictly following the <a href='https://2023.aclweb.org/calls/main_conference/#paper-submission-policies'>ACL 2023 style templates</a>, with the mandatory limitation section not counting towards the page limit. Supplementary and appendices (either as separate files or appended after the main submission) are allowed. We encourage code link submissions for the camera-ready version.

  <h3 class="sub_section_title"><strong>Dual Submission</strong></h3>

NewSumm 2023 will allow double submission as long as the authors make a decision before camera-ready. We will not consider any paper that overlaps significantly in content or results with papers that will be (or have been) published elsewhere. Authors submitting more than one paper to NewSumm 2023 must ensure that their submissions do not overlap significantly (>25%) with each other in content or results. Authors can submit up to 100 MB of supplementary materials separately. Authors are highly encouraged to submit their codes for reproducibility purposes. 


  <h3 class="sub_section_title"><strong>Fast-Track Submission</strong></h3>
If your paper has been reviewed by ACL, EMNLP, EACL, or ARR and the average rating is higher than 2.5 (either avg soundness or excitement score), the paper is qualified to be submitted to the fast-track. In the appendix, please include the reviews and a short statement discussing what parts of the paper have been revised.


ACL Rolling Review (ARR) Submissions: Our workshop also welcomes submissions from ARR. Authors of any papers that are submitted to ARR and have their meta review ready may submit their papers and reviews for consideration for the workshop until 10 October 2023. This should include submissions to ARR for the 15 August deadline. The decision of publication will be announced by 17 October 2023. The commitment should be done via the workshop submission website: <a href='https://www.softconf.com/emnlp2023/newsumm2023/'>START/SoftConf submission portal</a> ("ACL Rolling Review Commitment" submission type)


<h3 class="sub_section_title"><strong>Non-archival Option</strong></h3>


ACL workshops are traditionally archival. To allow dual submission of work, we are also including a non-archival track. Authors have the flexibility to submit their unpublished research in a non-archival format, where only the abstract will be included in the conference proceedings. These non-archival submissions are expected to meet the same quality criteria as their archival counterparts and will undergo an identical review process. This option is designed to facilitate future publication opportunities in journals or conferences that disallow previously archived material. It also aims to foster engagement and constructive feedback on well-developed but yet-to-be-published work. Like archival submissions, non-archival entries must conform to the established formatting and length guidelines.


<h2 class="blackpar_title" id="Dates">Important Dates:</h2>

 - <strong>Sep.8, 2023</strong>: Workshop Submission Due Date (extended from Sep. 1st)

 - <strong>Oct. 10, 2023</strong>: Fast-Track Submission and ARR Commitment Deadline 

- <strong>Oct. 17, 2023</strong>: Notification of Acceptance (Direct, ARR, and Fast-Track Notification)

 - <strong>Oct. 24, 2023</strong>: Camera-ready Papers Due

 - <strong>Dec. 6</strong>: Workshop Date



<!-- Organizers -->
<h2 class="blackpar_title" id="Organizers">Organizers</h2>
<div class="row">

 <div class="card column" >
	  <img src="/2023/images/Yue_Dong.jpg" alt="Yue Dong" class="img_card">
	  <div class="container">
		<center>
		<h4>
      <a href="https://www.cs.mcgill.ca/~ydong26/"><b>Yue Dong</b></a>
			<br>
			University of California, Riverside, USA
		</h4>
		</center>
	  </div>
	</div>

 <div class="card column" >
	  <img src="/2023/images/wen_xiao.jpg" alt="Wen Xiao" class="img_card">
	  <div class="container">
		<center>
		<h4>
      <a href="https://wendy-xiao.github.io/"><b>Wen Xiao</b></a>
			<br>
			University of British Columbia, Canada
		</h4>
		</center>
	  </div>
	</div>
 
    <div class="card column" style="margin-left:0;">
	  <img src="/2023/images/lu_wang.JPG" alt="Lu Wang" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<a href="https://web.eecs.umich.edu/~wangluxy/"><b>Wang Lu</b></a>
			<br>
			University of Michigan, USA
		</h4>
		</center>
	  </div>
	</div>
  </div>

<div class="row">
  <div class="card column" >
  <img src="/2023/images/fei_liu.JPG" alt="Fei Liu" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<a href="http://www.cs.ucf.edu/~feiliu/"><b>Fei Liu</b></a>
			<br>
			Emory University, USA
		</h4>
		</center>
	  </div>
	</div>
  

<div class="card column">
	  <img src="/2023/images/giuseppe_carenini.JPG" alt="Giuseppe Carenini" class="img_card">
	  <div class="container" >
		<center>
		<h4>
			<a href="https://www.cs.ubc.ca/~carenini/"><b>Giuseppe Carenini</b></a>
			<br>
			University of British Columbia, Canada
		</h4>
		</center>
	  </div>
	</div>
  
<!-- <div class="card column">
	  <img src="/2023/images/jackie_cheung.JPG" alt="Jackie Chi Kit Cheung" class="img_card">
	  <div class="container">
		<center>
		<h4>
			<a href="https://www.cs.mcgill.ca/~jcheung/"><b>Jackie Chi Kit Cheung</b></a>
			<br>
			McGill University & MILA, Canada
		</h4>
		</center>
	  </div>
	</div> -->
</div>

  
  
  
  
<!-- Confirmed Spearkers -->
<h2 class="blackpar_title" id="Confirmed Spearkers">Confirmed Spearkers</h2>
We are deeply sad that our invited speaker - Dragomir Radev had passed away in 2023. It's a profound loss that we cannot have him grace our stage. However, we are grateful that Drago's graduated PhD student, Rui Zhang, who is now an assistant professor at PSU, kindly accepted our invitation to give a keynote talk on summarization in memory of Drago. 

<ul>
<li>Kathleen McKeown (Columbia University)</li>
<li>Wojciech Kryscinski (Cohere)</li>
<li>Jackie Cheung (McGill University) </li>
<li>Iz Beltagy (AI2) </li>
<li>Chenguang Zhu (Zoom)</li>
<li>Rui Zhang (Penn State University) </li>	
<li>Dragomir Radev (in memory of, Yale University)</li>
</ul>



<!-- Schedule -->
<h2 class="blackpar_title" id="Schedule"><b>Schedule</b></h2>
TBA
<!-- 
<a href="https://summarization-workshop.github.io/schedule/"><b>NewSumm 2023 schedule (9am - 6pm AST)</b></a>
-->




<!-- PC -->
<h2 class="blackpar_title" id="Program Committee">Program Committee</h2>
<ul>
<li>Manabu Okumura (Tokyo Institute of Technology)</li>
<li>Ido Dagan (Bar-Ilan University)</li>
<li>Ming Zhong (UIUC)</li>
<li>Kristjan Arumae (Qualtrics)</li>
<li>Pengcheng He (Microsoft Research)</li>
<li>Naoaki Okazaki (Tokyo Institute of Technology)</li>
<li>Zhe Hu (Baidu Inc)</li>
<li>Wojciech Kryscinski (Salesforce Research)</li>
<li>Haopeng Zhang (University of California Davis)</li>
<li>Hou Pong Chan (University of Macau)</li>
<li>Yang Liu (Microsoft)</li>
<li>Kaiqiang Song (Tencent AI Lab)</li>
<li>Juan-Manuel Torres-Moreno (LIA Avignon Université)</li>
<li>Jing Jiang (Singapore Management University)</li>
<li>Ziqiang Cao (Soochow University)</li>
<li>Margot Mieskes (University of Applied Sciences, Darmstadt)</li>
<li>Felice Dell'Orletta (Istituto di Linguistica Computazionale «A. Zampolli», CNR, Pisa, Italy)</li>
<li>Xinnuo Xu (University of Edinburgh)</li>
<li>Richard Evans (University of Wolverhampton)</li>
<li>Esau Villatoro-Tello (Idiap Research Institute)</li>
<li>Susana Bautista (Universidad Francisco de Vitoria)</li>
<li>Tobias Falke (Amazon Alexa)</li>
<li>Kellie Webster (Google)</li>
<li>Giulia Venturi (Institute for Computational Linguistics "A. Zampolli" (ILC-CNR)</li>
<li>Jessica Ouyang (University of Texas at Dallas)</li>
<li>Wencan Luo (Google)</li>
<li>Rui Zhang (Penn State University)</li>
<li>Linzi Xing (University of British Columbia)</li>
<li>Jiacheng Xu (Salesforce AI Research)</li>
<li>Tadashi Nomoto (National Institute of Japanese Literature)</li>
<li>Chao Zhao (UNC Chapel Hill)</li>
<li>Ori Shapira (Amazon)</li>
<li>Patrick Huber (UBC)</li>
<li>Florian Boudin (Nantes Université)</li>
<li>Xinyu Hua (Bloomberg)</li>
<li>Elena Lloret (University of Alicante, Spain)</li>
<li>Alexander Fabbri (Salesforce AI Research)</li>
<li>Tanya Goyal (UT Austin)</li>
<li>Yuntian Deng (Harvard University)</li>
<li>Maxime Peyrard (EPFL)</li>
</ul>




